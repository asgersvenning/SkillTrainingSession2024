---
title: "Why use GAMs for (G)LMs?"
author: "Asger Svenning"
format: pdf
header-includes:
  - \usepackage{amsmath, amssymb, amsthm}
  - \usepackage{tabularx}
  - \usepackage{nicefrac}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 10, fig.height = 10, fig.align = "center", out.width = "90%")
library(tidyverse)
library(mgcv)
library(gratia)

library(ggforce)
library(furrr)
library(future)

theme_set(
  theme_bw() +
    theme(
      text = element_text(family = "serif"),
      strip.text = element_text(hjust = 0.5, size = 16, face = "bold"),
      strip.text.y.right = element_text(angle = 0),
      strip.text.y.left = element_text(angle = 0),
      title = element_text(hjust = 0.5, size = 16, face = "bold"),
      plot.title = element_text(size = 20, face = "plain"),
      legend.title = element_text(hjust = 0.5),
      legend.text = element_text(size = 14),
      panel.border = element_rect(color = "black", linewidth = 1)
    )
)
```

```{r, fig.width=15, fig.height=15}
iris_mod <- iris %>% 
  gam(
    Petal.Length ~ s(Sepal.Length, Species, bs = "fs"), data = .
  )

iris_expanded <- iris %>% 
  {expand_grid(
    Sepal.Length = seq(min(.$Sepal.Length), max(.$Sepal.Length), length.out = 100),
    Species = unique(.$Species)
  )}
  
iris_pred <- iris_expanded %>% 
    mutate(
        p = predict(iris_mod, ., type="link", se=T) %>% lapply(as.vector) %>% as_tibble
    ) %>% 
    unnest(
        p
    ) %>% 
    mutate(
        mid = fit,
        low = fit - se.fit * 1.96,
        high = fit + se.fit * 1.96,
        across(c(mid, low, high), inv_link(iris_mod))
    )

iris_post <- iris_expanded %>% 
  add_fitted_samples(iris_mod, n = 100)

iris %>% 
  ggplot(aes(Sepal.Length, Petal.Length)) +
  geom_point(
    aes(fill = Species),
    shape = 21,
    color = "black",
    stroke = 1,
    size = 3
  ) +
  geom_ribbon(
    data = iris_pred,
    aes(y = mid, ymin = low, ymax = high, color = Species),
    alpha = 0.15,
    linewidth = 0.5
  ) +
  geom_line(
    data = iris_pred,
    aes(y = mid, color = Species),
    linewidth = 2
  ) +
  geom_line(
    data = iris_post,
    aes(y = .fitted, color = Species, group = paste0(.draw, Species)),
    alpha = 0.25,
    linewidth = 0.5
  ) +
  scale_color_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  coord_cartesian(expand = F) +
  theme(
    aspect.ratio = 1,
    legend.position = "none",
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.margin = margin(),
    plot.background = element_blank(),
    panel.grid = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
  )
```
\clearpage

# Introduction
In this short example I hope to convince you that `mgcv` and the `gam` function is useful for more than just GAMs, but can - and indeed should - often be used in the case where the relationship under interest is linear (i.e. where a LM would usually be used).

I will show an example of how an unknown non-linear confounding variable can easily be accounted for using a GAM, while either including it or not in a standard LM will lead to biased (incorrect) estimates - which in the worst case can lead to opposite conclusions.

## Sampling a smooth confounding curve
To illustrate the problem, we will first need to define a way to generate a non-linear function. Later we will sample a new random non-linear function for each experiment, which is then used as the relationship between the confounding variable and the response. 

```{r}
plot_smooth <- function(sfun, z, s, zlim) {
  plt <- tibble(
    z = z,
    s = s
  ) %>% 
    ggplot(aes(z, s)) +
    geom_function(fun = sfun, xlim = zlim, color = "red", linewidth = 1) +
    geom_point(size = 2) +
    labs(title = latex2exp::TeX("Control points and curve for one instance of $f_s$"))
  
  print(plt)
}
```

To do this, we generate $N$ evenly spaced control points, $c$, along the $z$-axis, and then sample $N$ random values from a uniform distribution to generate the $s$-values. The position of the control points on the $z$-axis is then subtracted from the $s$-values to give a slight overall negative trend on average:
\begin{align*}
  c_z &\sim \mathcal{U}(z_{\mathrm{min}}, z_{\mathrm{max}}), \quad\quad\;\;\; z_{\mathrm{min}} = -0.1, \quad z_{\mathrm{max}} = 1.1 \\
  c_s &\sim \mathcal{U}(s_{\mathrm{min}}, s_{\mathrm{max}}) - z, \quad s_{\mathrm{min}} = -1, \quad\;\;\; s_{\mathrm{max}} = 1
\end{align*}
This is done to shift the distribution of confounding functions in such a way that the confounding effect is more likely to be opposite the true effect of $x$ on $y$, which for this experiment will be positive one-to-one relationship. 

We then use the `splinefun` function to generate a function, $f_s$, that smoothly interpolates between the control points:
$$f_s(z) = \text{splinefun}(c_z, c_s, \text{method} = \text{"natural"})$$
\clearpage

```{r, echo=T, fig.cap="Code for sampling a random non-linear function for the confounding variable."}
get_sfun <- function(
    zlim = c(-0.1, 1.1), 
    slim = c(-1, 1), 
    ctrl.pts=10, 
    plot=F
  ) {
  z <- seq(zlim[1], zlim[2], length.out = ctrl.pts)
  c <- runif(ctrl.pts, min = slim[1], max = slim[2]) - z

  sfun <- splinefun(z, c, method = "natural")
  if (plot) plot_smooth(sfun, z, c, zlim)
  
  return(invisible(sfun))
}
```

### Example smooth confounding curve
The following plot shows an example of a smooth confounding curve generated using the `get_sfun` function.
```{r, fig.height=3, fig.width=6}
set.seed(126)
get_sfun(plot=T)
```

\clearpage

# The experiment

```{r}
plot_data <- function(data) {
  plt <- data %>%
    ggplot() +
    geom_autopoint(size=0.25) +
    geom_autodensity(color = "black", linewidth = 0.75, fill = "gray75") +
    facet_matrix(vars(x, z, y), vars(x, z, y), layer.diag = 2) +
    labs(
      title = "Raw relationship between the observed\n (x, z) and response (y) variables"
    ) 
  
  return(invisible(plt))
}

plot_models <- function(gam_mod, lin_mod1, lin_mod2, data_plot) {
  rm_titles <- theme(
    axis.title = element_blank(),
    plot.title = element_blank(),
    plot.subtitle = element_blank()
  )
  
  p1 <- appraise(lin_mod2, point_alpha=0.5) &
    rm_titles &
    patchwork::plot_annotation(
      title = "Linear model (y ~ x)",
      # subtitle = str_wrap("Omitting z from the model results in a malformed model due to the confounding effect of z on y", 40),
      theme = theme_get()
    )
  
  p2 <- appraise(lin_mod1, point_alpha=0.5) &
    rm_titles &
    patchwork::plot_annotation(
      title = "Linear model (y ~ x + z)",
      # subtitle = str_wrap("Including z as a linear term catastrophically fails to capture the confounding effect of z on y", 40),
      theme = theme_get()
    )
  
  p3 <- appraise(gam_mod, point_alpha=0.5) &
    rm_titles &
    patchwork::plot_annotation(
      title = "GAM model (y ~ x + s(z))",
      # subtitle = str_wrap("The smooth term s(z) captures the confounding effect of z on y, and yields a well-behaved model", 40),
      theme = theme_get()
    )
  
  cps <- list(data_plot, p1, p2, p3) %>% 
    # lapply(ggplot2::ggplotGrob) %>%
    lapply(patchwork::wrap_elements) %>% 
    patchwork::wrap_plots() 
  
  print(cps)
  
  return(invisible())
}
```

We will now define a simple experiment where we generate a dataset with three variables: $x$, $z$, and $y$. Both $x$ and $z$ are generated as a linear function of a latent variable $l$, with a small amount of uniform noise added. This also means that $x$ and $z$ are highly collinear (in this case the correlation coefficient $\rho \approx 0.95$), a case where common wisdom is to exclude one of the variables from the model to avoid multicollinearity. However, as we will see later, in the case of confounding variables excluding the variable can lead to biased estimates, and thus incorrect conclusions.

The response variable $y$ is then generated as a linear function of $x$, a randomly sampled non-linear function of $f_s(z)$, and some normally distributed noise:
$$
y = x + 3 \cdot f_s(z) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \nicefrac{1}{8})
$$
The specific form of the predictor-response relationship was chosen to ensure that the effect of the confounding variable, $z$, is stronger than the effect of the predictor of interest, $x$, while the noise is kept small enough that residual patterns in the diagnostic plots are very clear.

We then fit three models to the data: 

* A GAM model with $y$ as a function of $x$ and a smooth term for $z$: `gam(y ~ x + s(z))`.
* A linear model with $y$ as a function of $x$ and $z$: `lm(y ~ x + z)`.
* A linear model with $y$ as a function of $x$ only: `lm(y ~ x)`.

The `experiment` function below generates the data, fits the models, and returns the estimated coefficient for $x$ and the standard error for each model. We also calculate the error (difference between the estimated coefficient and the true value of 1) and the z\textsubscript{error}-score, $\nicefrac{\tilde{\mu_x} - \mu_x}{\widetilde{\sigma(\mu_x)}}$, for each model. 

Note: It is very important to take notice of the fact that $f_s$ is a random non-linear function, and that for each iteration of the experiment a new $f_s$ is sampled. This is done to ensure that the results hold in general for non-linear confounding variables, and not just for a specific shape of $f_s$.

\clearpage

```{r, echo=T, fig.cap="Code for executing one iteration of the experiment."}
experiment <- function(plot=F) {
  data <- tibble(
    # Sample the latent variable
    l = runif(500),
    # Generate x and z as linear functions of l with a small amount of
    # independent uniform noise to ensure they are highly, but not
    # perfectly, collinear
    x = l + runif(500, -1, 1) / 10,
    z = l + runif(500, -1, 1) / 10,  
    # Generate y as a linear function of x and a sampled non-linear function
    # of z, with some normally distributed noise
    y = x + 5 * get_sfun()(z) + rnorm(500, 1/8)
  )
  
  if (plot) dp <- plot_data(data)
  
  # Fit the three models
  gam_mod <- bam(y ~ x + s(z), data = data, method = "fREML", discrete=T)
  lin_mod1 <- lm(y ~ x + z, data = data)
  lin_mod2 <- lm(y ~ x, data = data)
  
  if (plot) plot_models(gam_mod, lin_mod1, lin_mod2, dp)
  
  # Extract and summarize the results for each model
  res <- tibble(
    type = c("gam", "lin1", "lin2"),
    model = list(gam_mod, lin_mod1, lin_mod2),
    result = map(model, ~ broom::tidy(.x, parametric=T))
  ) %>% 
    select(!model) %>% 
    unnest(result) %>% 
    select(type, term, estimate, std.error) %>% 
    filter(term == "x") %>% 
    mutate(
      error = estimate - 1,
      z.error = error / std.error
    )
  
  return(invisible(res))
}
```

\clearpage

### Example diagnostics
Before we dive into the results, let's look at the diagnostics for the three models fitted to a particular instantiation of the data.
```{r, fig.height=10, fig.width=10, fig.cap="(TR, BL \\& BR) Diagnostic plots for the three models with sub-panels; QQ-plot (TL), residuals-fitted/linear predictor (TR), residual histogram (BL) and observed-fitted (BR). (TL) Pairwise scatter plot and histograms for x, y and z."}
set.seed(112)
experiment(T)
```
As can be clearly be seen by inspecting the diagnostic plots, the GAM model captures the relationship between $y$ and $z$ well, leading to acceptable model diagnostics, whereas the linear models would have to be rejected based on the diagnostics. However, does this also result in incorrect estimates of the coefficient for $x$ ($\mu_x$)? 

# Results
To answer the question about whether the GAM model leads to better estimates of the coefficient for $x$, we will run the `experiment` function 2000 times and compare the results for the three models. Remember that for each iteration we sample a new smooth function $f_s$ and generate new data, this is done to ensure that the results hold in general for non-linear confounding variables.
```{r}
plan(multisession, workers=12)
rep_exp <- future_map(1:2000, function(dummy) {
  experiment()
}, .progress = T, .options = furrr_options(seed = 123)) %>% 
  bind_rows()

plan(sequential)
```

### Summary of results
The table below shows the average estimated coefficient for $x$ ($\tilde{\mu_x}$), the mean absolute error ($\overline{\lvert \tilde{\mu_x} - \mu_x\rvert}$), the rate of confident opposite conclusions ($\overline{\mathrm{sign}(\tilde{\mu_x}) \neq \mathrm{sign}(\mu_x) \land \left\lvert \nicefrac{\tilde\mu_x}{\widetilde{\sigma(\mu_x)}} \right\rvert > 1.96}$), and the rate of estimates within the confidence interval ($\nicefrac{\lvert \tilde{\mu_x} - \mu_x\rvert}{\widetilde{\sigma(\mu_x)}} < 1.96$) for the three models.
```{r, results="asis"}
signif_rate <- rep_exp %>% 
    group_by(type) %>% 
    summarize(
        signif = mean(abs(estimate/std.error) > 1.96)
    )

sr_gam <- signif_rate %>% 
    filter(type == "gam") %>% 
    pull(signif)

sr_lin1 <- signif_rate %>%
    filter(type == "lin1") %>% 
    pull(signif)

sr_lin2 <- signif_rate %>%
    filter(type == "lin2") %>% 
    pull(signif)

table_headers <- c(
  "type" = "Model",
  "mu" = "Mean\\\\estimated slope",
  "mae" = "Mean\\\\absolute error",
  "conf_inc" = "Rate of confident\\\\opposite conclusions",
  "mu_cf" = "Rate of estimate\\\\within\\\\confidence interval"
)

rep_exp %>% 
  mutate(
    type = case_when(
      type == "gam" ~ "$y \\sim x + s(z)$",
      type == "lin1" ~ "$y \\sim x + z$",
      type == "lin2" ~ "$y \\sim x$"
    ) %>% 
      factor(levels = c("$y \\sim x + s(z)$", "$y \\sim x + z$", "$y \\sim x$"))
  ) %>% 
  group_by(type) %>% 
  summarise(
    mu = mean(estimate),
    mae = mean(abs(error)),
    conf_inc = mean(estimate <= 0 & abs(estimate/std.error) > 1.96),
    mu_cf = mean(abs(z.error) < 1.96),
  ) %>% 
  mutate(
    across(!c(type, mu, mae), function(num) {
      num %>% 
        scales::percent_format(accuracy = 0.01)() %>% 
        str_replace("%", "\\\\%")
    })
  ) %>% 
  kableExtra::kable(
    format = "latex",
    booktabs = T,
    tabular = "tabularx",
    escape = F, 
    col.names = magrittr::set_names(paste0("\\makecell{", table_headers, "}"), names(table_headers)),
    align = "lcccc", 
    digits = 2
  ) %>% 
  kableExtra::kable_paper(
    full_width = F
  ) %>%
  cat
```

The three models also differ in the rate of rejecting the null hypothesis of $\mu_x=0$, with the GAM model rejecting the null hypothesis in `r round(sr_gam, 2)*100`% of the cases, the linear model with $y \sim x + z$ rejecting the null hypothesis in `r round(sr_lin1, 2)*100`\% of the cases, and the linear model with $y \sim x$ rejecting the null hypothesis in `r round(sr_lin2, 2)*100`\% of the cases.

### Expected versus observed error distribution
The following plot shows the distribution of the z\textsubscript{error}-score, $\nicefrac{\tilde{\mu_x} - \mu_x}{\widetilde{\sigma(\mu_x)}}$, for the three models. The blue line shows the expected distribution of the z\textsubscript{error}-score under the hypothesis of $\mu_x=1$, i.e. a standard normal distribution. The green dashed line shows the median z\textsubscript{error}-score for each model, and the red line shows the expected value of the z\textsubscript{error}-score (0).
```{r, fig.height=5, fig.cap="Distribution of the $z_error$-score, $\\nicefrac{\\tilde{\\mu_x} - \\mu_x}{\\widetilde{\\sigma(\\mu_x)}}$, for the three models, with the expected distribution under the null hypothesis of $\\mu_x=1$ (blue), the median $z_error$-score (green dashed) and the expected value of the $z_error$-score (red)."}
rep_exp %>% 
  mutate(
    type = case_when(
      type == "gam" ~ "GAM: y ~ x + s(z)",
      type == "lin1" ~ "LM: y ~ x + z",
      type == "lin2" ~ "LM: y ~ x"
    ) %>% 
      factor(levels = c("GAM: y ~ x + s(z)", "LM: y ~ x + z", "LM: y ~ x"))
  ) %>% 
  ggplot(aes(z.error)) +
  geom_histogram(
    aes(y = after_stat(ncount)), 
    bins = 50,
    color = "gray45",
    fill = "gray45"
  ) +
  geom_function(
    fun = ~dnorm(., 0, 1) / dnorm(0, 0, 1), 
    col = "blue",
    linewidth = 1
  ) +
  geom_vline(
    xintercept = 0, 
    col = "red",
    linewidth = 1
  ) +
  stat_summary(
    aes(y = 0.5, xintercept = after_stat(x)), 
    fun = median, 
    orientation = "y",
    geom = "vline", 
    color = "green",
    linewidth = 1,
    linetype = "dashed"
  ) +
  scale_y_continuous(expand = expansion(c(0, 0.1))) +
  facet_wrap(~type, scales = "free_x") 
```

# Conclusion
As clearly demonstrated the GAM model accurately accounts for the non-linear confounding variable, leading to unbiased estimates of the coefficient for $x$ and accurate confidence intervals. The purely linear models, on the other hand, fail to account for the confounding variable, leading to incorrect confidence intervals in the average case, and in the worst case, opposite conclusions. It also demonstrates how properly modelling confounding variables can lead to higher statistical power, even though the model is using more degrees of freedom.

This simple example illustrates how GAMs can be used to account for non-linear confounding variables, even when the relationship of interest is linear. This is especially important in observational studies where the relationship between the confounding variable and the response is unknown, and thus cannot be accounted for using a linear model.